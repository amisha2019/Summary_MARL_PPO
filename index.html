<!DOCTYPE html>
<html>
    <head>
      <meta charset="utf-8">
      <meta name="description"
            content="PPO and MARL">
      <meta name="keywords" content="PPO, MARL">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <title>An Ultimate Summary of PPOs in Multi-agent Reinforcement Learning </title>
    
      <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
      
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
      <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
      <!-- <script>
        window.dataLayer = window.dataLayer || [];
    
        function gtag() {
          dataLayer.push(arguments);
        }
    
        gtag('js', new Date());
    
        gtag('config', 'G-PYVRSFMDRL');
      </script> -->
    
      <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
            rel="stylesheet">
    
      <link rel="stylesheet" href="./static/css/bulma.min.css">
      <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
      <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
      <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
      <link rel="stylesheet"
            href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
      <link rel="stylesheet" href="./static/css/index.css">
      <link rel="icon" href="./static/images/favicon.svg">
    
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
      <script defer src="./static/js/fontawesome.all.min.js"></script>
      <script src="./static/js/bulma-carousel.min.js"></script>
      <script src="./static/js/bulma-slider.min.js"></script>
      <script src="./static/js/index.js"></script>
      <script src="https://unpkg.com/htmlincludejs"></script>
    
    </head>

    <body>
      <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <a class="navbar-item" href="http://raaslab.org/">
          <span class="icon">
              <i class="fas fa-home"></i>
          </span>
          </a>
    
          <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                  Table of Contents
                </a>
                <div class="navbar-dropdown">
                  <a class="navbar-item" href="#Introduction">
                    Introduction
                  </a>
                  <a class="navbar-item" href="#Problem">
                    Problem Formation
                  </a>
                  <a class="navbar-item" href="#Training Method">
                    Training Methods
                  </a>
                  <a class="navbar-item" href="#PPO">
                    PPO
                  </a>
                  <a class="navbar-item" href="#IPPO">
                    IPPO
                  </a>
                  <a class="navbar-item" href="#MAPPO">
                    MAPPO
                  </a>
                  <a class="navbar-item" href="#HAPPO">
                    HAPPO
                  </a>
                  <a class="navbar-item" href="#relatedWorks">
                    Related Works
                  </a>
                  <a class="navbar-item" href="#Reference">
                    Reference
                  </a>
                </div>
              </div>
        
            </div>
        
          </div>
        </nav>


    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">An Ultimate Summary of PPO in Multi-agent Reinforcement Learning</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="">Chak Lam Shek</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="">Amisha</a><sup>*</sup>,</span>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    
    
    
    <section class="hero is-light" >
      <div class="hero-body">
        <div class="container has-text-justified">
         <h2 class="title is-3 has-text-centered"></br><a id="Introdyction">Introduction</a></h2>
         <p>  </p>

           
           <div class="container has-text-justified">
            <h3 class="title is-3 has-text-left"></br>Multi-agent Reinforcement Learning (MARL)</h2>
            <p>  </p>
              </br>

           </div>
           
           <div class="container has-text-justified">
            <h3 class="title is-3 has-text-left"></br>Relationship Between Decision-Making for robootics</h2>
            <p>  </p>
              </br>

           </div>
           
           <div class="container has-text-justified">
            <h3 class="title is-3 has-text-left"></br>Research Areas</h2>
            <p>  </p>
              </br>

           </div>
           
        </div>
      </div>
    </section>
    
    <section class="hero" >
      <div class="hero-body">
        <div class="container has-text-justified">
         <h2 class="title is-3 has-text-centered"></br><a id="Problem">Problem Formation</a></h2>
         <p> A common problem of MARL is formulated as Decentralized Partially Observable Markov Decision Process (<b>Dec-POMDP</b>), an extension of the standard Markov Decision Process (MDP) Model. Dec-POMDP can be described as the tuple $$ G= (N, \textbf{S},\textbf{A},P,R, \Omega, \textbf{O}, \gamma )$$ The defination of each elemnt is presented below.</p>
           </br>
        <ul style="list-style-type:disc">
             <li> Number of agents \((N)\) </li>
             <li> States \( (\textbf{S}) = (s_1, s_2, s_3, ..., s_n) \)  is the joint state space of the agents. </li>
             <li> Action \( (\textbf{A}) = (a_1, a_2, a_3, ..., a_n) \) is the joint action space of the agents.</li>
             <li> Transition Probabilities \( P(\textbf{s}'|\textbf{s}, \textbf{a}) : \textbf{S} \times \textbf{A} \rightarrow \textbf{S} \) specifies the probability of transitioning from joint state \( \textbf{s} \) to state \( \textbf{s}' \) given a joint action \( \textbf{a} \) by all agents. </li>
             <li> Reward function \( R(\textbf{s}, \textbf{a}) : \textbf{S} \times \textbf{A} \rightarrow \mathbb{R} \) defines the immediate rewards agents received for taking joint action \( \textbf{a} \) in joint state \( \textbf{s} \). </li>
             <li> Observation space \((\Omega\)) </li>
             <li> Obervation probabilities \(( \textbf{O}) \) defines the observation model for each agent, which specifies the probability of receiving observation when taking joint action \( \textbf{a} \) by all agents in the joint state \( \textbf{s} \). </li>
             <li> Discount factor \( (\gamma)\)
         </ul>
        </br>
        <p> </p>
        </div>
      </div>
    </section>
    
    <section class="hero is-light" >
      <div class="hero-body">
        <div class="container has-text-justified">
         <h2 class="title is-3 has-text-centered"></br><a id="Training Method">Training Methods</a></h2>
         <p>  </p>
           </br>

        </div>
      </div>
    </section>
    
    
    
    <section class="hero" >
      <div class="hero-body">
        <div class="container has-text-justified">
         <h2 class="title is-3 has-text-centered"></br><a id="PPO">PPO</a></h2>
         <p>  </p>
           </br>

        </div>
      </div>
    </section>
    
    <section class="hero is-light" >
      <div class="hero-body">
        <div class="container has-text-justified">
         <h2 class="title is-3 has-text-centered"></br><a id="IPPO">IPPO</a></h2>
         <p>  </p>
           </br>

        </div>
      </div>
    </section>
    
    
    <section class="hero " >
      <div class="hero-body">
        <div class="container has-text-justified">
         <h2 class="title is-3 has-text-centered"></br><a id="MAPPO">MAPPO</a></h2>
         <p>  </p>
           </br>

        </div>
      </div>
    </section>
    
    <section class="hero is-light" >
      <div class="hero-body">
        <div class="container has-text-justified">
         <h2 class="title is-3 has-text-centered"></br><a id="HAPPO">HAPPO</a></h2>
         <p>  </p>
           </br>

        </div>
      </div>
    </section>
    
    
    <section class="hero " >
      <div class="hero-body">
        <div class="container has-text-justified">
         <h2 class="title is-3 has-text-centered"></br><a id="relatedWorks">Related Works</a></h2>
         <p>  </p>
           </br>

        </div>
      </div>
    </section>
    
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Concurrent Work. -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3"><a id="Reference">Reference</a></h2>

            <div class="content has-text-justified">
              <p>
                <a href="https://arxiv.org/pdf/1502.05477.pdf">Trust Region Policy Optimization</a>. John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. ICML. 2015
              </p>
              <p>
                <a href="https://arxiv.org/pdf/1707.06347.pdf">Proximal Policy Optimization Algorithms</a>. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. OpenAI 2017.
              </p>

              <p>
                <a href="https://arxiv.org/pdf/2011.09533.pdf">Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?</a>. Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip H.S. Torr, Mingfei Sun, Shimon Whiteson. 2020
              </p>
              <p>
                <a href="https://arxiv.org/pdf/2103.01955.pdf">The Surprising Effectiveness of PPO in Cooperative Multi-Agent Game</a>. Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, Yi Wu. NeurlPS 2022
              </p>
              <p>
                <a href="https://arxiv.org/pdf/2109.11251.pdf">TRUST REGION POLICY OPTIMISATION IN MULTI-AGENT REINFORCEMENT LEARNING</a>. Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, Yaodong Yang. ICRL 2022.
              </p>
              <p>
                <a href="https://arxiv.org/pdf/2205.14953.pdf">Multi-Agent Reinforcement Learning is A Sequence Modeling Problem</a>. Muning Wen, Jakub Grudzien Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, Yaodong Yang. 2022.
              </p>
              <p>
                <a href="https://arxiv.org/pdf/2203.11653.pdf">Transferring Multi-Agent Reinforcement Learning Policies for Autonomous Driving using Sim-to-Real</a>. Eduardo Candela, Leandro Parada, Luis Marques, Tiberiu-Andrei Georgescu, Yiannis Demiris, Panagiotis Angeloudis. 2022.
              </p>
              <p>
                <a href="https://arxiv.org/pdf/2304.09870.pdf">Heterogeneous-Agent Reinforcement Learning</a>. Yifan Zhong, Jakub Grudzien Kuba, Siyi Hu, Jiaming Ji, Yaodong Yang. 2023.
              </p>
              <p>
                <a href="https://arxiv.org/pdf/1803.11485.pdf">QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning</a>. Tabish Rashid, Mikayel Samvelyan, Christian Schroeder de Witt, Gregory Farquhar, Jakob Foerster, Shimon Whiteson. ICML 2018.
              </p>
              <p>
                <a href="https://arxiv.org/pdf/1706.02275.pdf">Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments</a>. Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch. 2020.
              </p>
              <p>
                <a href="https://web.media.mit.edu/~cynthiab/Readings/tan-MAS-reinfLearn.pdf">Multi-Agent Reinforcement Learning: Independent vs. Coop erative Agents</a>. Ming Tan.
              </p>
              <p>
                <a href="https://arxiv.org/pdf/1706.05296.pdf">Value-Decomposition Networks For Cooperative Multi-Agent Learning</a>. Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, Thore Graepel. 2017.
              </p>
            </div>
          </div>
        </div>

      </div>
    </section>
    </body>
</html>
